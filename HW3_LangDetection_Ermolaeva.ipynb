{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определение языка (language detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Скачаем немного википедии для тестов\n",
    "Воспользуемся пакетом *wikipedia*:\n",
    "\n",
    "`pip install wikipedia`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, какие вообще есть языки в википедии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "code2lang = wikipedia.languages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['aa', 'ab', 'ace', 'ady', 'ady-cyrl', 'aeb', 'aeb-arab', 'aeb-latn', 'af', 'ak', 'aln', 'als', 'am', 'an', 'ang', 'anp', 'ar', 'arc', 'arn', 'arq', 'ary', 'arz', 'as', 'ase', 'ast', 'atj', 'av', 'avk', 'awa', 'ay', 'az', 'azb', 'ba', 'ban', 'bar', 'bat-smg', 'bbc', 'bbc-latn', 'bcc', 'bcl', 'be', 'be-tarask', 'be-x-old', 'bg', 'bgn', 'bh', 'bho', 'bi', 'bjn', 'bm', 'bn', 'bo', 'bpy', 'bqi', 'br', 'brh', 'bs', 'bto', 'bug', 'bxr', 'ca', 'cbk-zam', 'cdo', 'ce', 'ceb', 'ch', 'cho', 'chr', 'chy', 'ckb', 'co', 'cps', 'cr', 'crh', 'crh-cyrl', 'crh-latn', 'cs', 'csb', 'cu', 'cv', 'cy', 'da', 'de', 'de-at', 'de-ch', 'de-formal', 'din', 'diq', 'dsb', 'dtp', 'dty', 'dv', 'dz', 'ee', 'egl', 'el', 'eml', 'en', 'en-ca', 'en-gb', 'eo', 'es', 'es-formal', 'et', 'eu', 'ext', 'fa', 'ff', 'fi', 'fit', 'fiu-vro', 'fj', 'fo', 'fr', 'frc', 'frp', 'frr', 'fur', 'fy', 'ga', 'gag', 'gan', 'gan-hans', 'gan-hant', 'gcr', 'gd', 'gl', 'glk', 'gn', 'gom', 'gom-deva', 'gom-latn', 'gor', 'got', 'grc', 'gsw', 'gu', 'gv', 'ha', 'hak', 'haw', 'he', 'hi', 'hif', 'hif-latn', 'hil', 'ho', 'hr', 'hrx', 'hsb', 'ht', 'hu', 'hu-formal', 'hy', 'hz', 'ia', 'id', 'ie', 'ig', 'ii', 'ik', 'ike-cans', 'ike-latn', 'ilo', 'inh', 'io', 'is', 'it', 'iu', 'ja', 'jam', 'jbo', 'jut', 'jv', 'ka', 'kaa', 'kab', 'kbd', 'kbd-cyrl', 'kbp', 'kg', 'khw', 'ki', 'kiu', 'kj', 'kk', 'kk-arab', 'kk-cn', 'kk-cyrl', 'kk-kz', 'kk-latn', 'kk-tr', 'kl', 'km', 'kn', 'ko', 'ko-kp', 'koi', 'kr', 'krc', 'kri', 'krj', 'krl', 'ks', 'ks-arab', 'ks-deva', 'ksh', 'ku', 'ku-arab', 'ku-latn', 'kum', 'kv', 'kw', 'ky', 'la', 'lad', 'lb', 'lbe', 'lez', 'lfn', 'lg', 'li', 'lij', 'liv', 'lki', 'lmo', 'ln', 'lo', 'loz', 'lrc', 'lt', 'ltg', 'lus', 'luz', 'lv', 'lzh', 'lzz', 'mai', 'map-bms', 'mdf', 'mg', 'mh', 'mhr', 'mi', 'min', 'mk', 'ml', 'mn', 'mo', 'mr', 'mrj', 'ms', 'mt', 'mus', 'mwl', 'my', 'myv', 'mzn', 'na', 'nah', 'nan', 'nap', 'nb', 'nds', 'nds-nl', 'ne', 'new', 'ng', 'niu', 'nl', 'nl-informal', 'nn', 'no', 'nov', 'nrm', 'nso', 'nv', 'ny', 'nys', 'oc', 'olo', 'om', 'or', 'os', 'pa', 'pag', 'pam', 'pap', 'pcd', 'pdc', 'pdt', 'pfl', 'pi', 'pih', 'pl', 'pms', 'pnb', 'pnt', 'prg', 'ps', 'pt', 'pt-br', 'qu', 'qug', 'rgn', 'rif', 'rm', 'rmy', 'rn', 'ro', 'roa-rup', 'roa-tara', 'ru', 'rue', 'rup', 'ruq', 'ruq-cyrl', 'ruq-latn', 'rw', 'sa', 'sah', 'sat', 'sc', 'scn', 'sco', 'sd', 'sdc', 'sdh', 'se', 'sei', 'ses', 'sg', 'sgs', 'sh', 'shi', 'shi-latn', 'shi-tfng', 'shn', 'si', 'simple', 'sk', 'skr', 'skr-arab', 'sl', 'sli', 'sm', 'sma', 'sn', 'so', 'sq', 'sr', 'sr-ec', 'sr-el', 'srn', 'ss', 'st', 'stq', 'sty', 'su', 'sv', 'sw', 'szl', 'ta', 'tay', 'tcy', 'te', 'tet', 'tg', 'tg-cyrl', 'tg-latn', 'th', 'ti', 'tk', 'tl', 'tly', 'tn', 'to', 'tpi', 'tr', 'tru', 'ts', 'tt', 'tt-cyrl', 'tt-latn', 'tum', 'tw', 'ty', 'tyv', 'tzm', 'udm', 'ug', 'ug-arab', 'ug-latn', 'uk', 'ur', 'uz', 'uz-cyrl', 'uz-latn', 've', 'vec', 'vep', 'vi', 'vls', 'vmf', 'vo', 'vot', 'vro', 'wa', 'war', 'wo', 'wuu', 'xal', 'xh', 'xmf', 'yi', 'yo', 'yue', 'za', 'zea', 'zh', 'zh-classical', 'zh-cn', 'zh-hans', 'zh-hant', 'zh-hk', 'zh-min-nan', 'zh-mo', 'zh-my', 'zh-sg', 'zh-tw', 'zh-yue', 'zu'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langs = code2lang.keys()\n",
    "langs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выберем 7 из них:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = ['tyv', 'ru', 'be', 'es', 'en', 'udm', 'de']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_texts_for_lang(lang, n): # функция для скачивания статей из википедии\n",
    "    wikipedia.set_lang(lang)\n",
    "    wiki_content = []\n",
    "    pages = wikipedia.random(n)\n",
    "    \n",
    "    for page_name in pages:\n",
    "        try:\n",
    "            page = wikipedia.page(page_name)\n",
    "        \n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "        wiki_content.append('{}\\n{}'.format(page.title, page.content.replace('==', '')))\n",
    "\n",
    "    return wiki_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tyv 100\n",
      "ru 90\n",
      "be 94\n",
      "es 96\n",
      "en 95\n",
      "udm 99\n",
      "de 93\n"
     ]
    }
   ],
   "source": [
    "wiki_texts = {}\n",
    "\n",
    "for lang in langs:\n",
    "    wiki_texts[lang] = get_texts_for_lang(lang, 100)\n",
    "    print(lang, len(wiki_texts[lang]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вуж Тӥгырмен\n",
      "Вуж Тӥгырмен (ӟуч нимыз Калашур) – Кияса ёрослэн удмурт гурт, интыяськемын ӝужыт гурезь бамалэ. Котырысь нюлэскын трос бoры, эмезь, губи.\n"
     ]
    }
   ],
   "source": [
    "print(wiki_texts['udm'][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anhalt-Bernburg-Schaumburg-Hoym\n",
      "Anhalt-Bernburg-Schaumburg-Hoym (originalmente Anhalt-Zeitz-Hoym) fue un principado alemán y un miembro del Sacro Imperio Romano Germánico. La muerte del Príncipe Víctor Amadeo de Anhalt-Bernburg en 1718, resultó en la partición de sus territorios, heredando su segundo hijo Lebrecht lo que originalmente era conocido como Anhalt-Zeitz-Hoym.\n",
      "En nombre del principado fue modificado en 1727 de Anhalt-Zeitz-Hoym a Anhalt-Bernburg-Schaumburg-Hoym.[1]​ La muerte del Príncipe Federico el 24 de diciembre de 1812 resultó en la extinción de la casa reinante y el territorios fue heredado por los príncipes de Anhalt-Bernburg.\n",
      "\n",
      "\n",
      " Príncipes de Anhalt-Zeitz-Hoym 1718-1727 \n",
      "Lebrecht 1718-1727\n",
      "Víctor I Amadeo Adolfo 1727\n",
      "El nombre del Principado es modificado a Anhalt-Bernburg-Schaumburg-Hoym\n",
      "\n",
      "\n",
      " Príncipes de Anhalt-Bernburg-Schaumburg-Hoym 1727-1812 \n",
      "Víctor I Amadeo Adolfo 1727-1772\n",
      "Carlos Luis 1772-1806\n",
      "Víctor II Carlos Federico 1806-1812\n",
      "Federico 1812\n",
      "A Anhalt-Bernburg\n",
      "\n",
      "\n",
      " Referencias \n"
     ]
    }
   ],
   "source": [
    "print(wiki_texts['es'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Очистка текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    data = data.lower()\n",
    "    data = re.sub(r'[^\\w\\s]', '', data)\n",
    "    data = re.sub('[0-9]+', '', data)\n",
    "    data = re.sub(r'[\\s]{2,}', ' ', data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = clean_data(text)\n",
    "    \n",
    "    return text.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Словарный метод"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import collections\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим частотный словарь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dict = {}\n",
    "\n",
    "for lang in langs:\n",
    "    freq_dict[lang] = collections.defaultdict(lambda: 0)\n",
    "    for lang_text in wiki_texts[lang]:\n",
    "        for word in tokenize(lang_text.replace('\\n', '').lower()):\n",
    "            freq_dict[lang][word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим абсолютную частотность каждого слова (частотность слова / частотность всех слов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for lang in langs:\n",
    "    for word in freq_dict[lang]:\n",
    "        freq_dict[lang][word] = freq_dict[lang][word] / sum(freq_dict[lang].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04664142530034972\tбудынкі\n",
      "0.04536692357662455\tкуалалумпур\n",
      "0.03965752235962006\tменара\n",
      "0.03505847230841297\tсамыя\n",
      "0.031292453691956074\tпетронас\n",
      "0.028164237176958498\tвежы\n",
      "0.02644421233470868\tміжнародны\n",
      "0.022101030530459965\tмалайзіі\n",
      "0.021397732884657076\tофісныя\n",
      "0.02120472872100268\tуніверсітэт\n",
      "0.019731878310762962\tтрэцяя\n",
      "0.01827746038525915\tвежа\n",
      "0.016998677372491867\tтэлевізійная\n",
      "0.015867133479555363\tкуалалумпурскі\n",
      "0.014860057642654986\tадкрыты\n",
      "0.013959004924432701\tмедыцынскі\n",
      "0.01314889639034022\tucsi\n",
      "0.01241730003177646\tісламскі\n",
      "0.011753885783848344\tмалаі\n",
      "0.010096637258789037\tklia\n"
     ]
    }
   ],
   "source": [
    "for word in sorted(freq_dict['be'], key=lambda w: freq_dict['be'][w], reverse=True)[:20]:\n",
    "    print('{}\\t{}'.format(freq_dict['be'][word], word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_language(text, freq_dict):\n",
    "    words_freq = {}\n",
    "    \n",
    "    test_text = tokenize(text)\n",
    "\n",
    "    for lang in langs:\n",
    "        i = 0\n",
    "        for word in test_text:\n",
    "            if word in freq_dict[lang]:\n",
    "                i += freq_dict[lang][word]\n",
    "                \n",
    "        words_freq[lang] = i   \n",
    "#     print(words_freq)\n",
    "    \n",
    "    return max(words_freq, key=words_freq.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'de'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_language('Welche Sprache ist das?', freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ru'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_language('Какой это язык?', freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'es'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_language('¿Qué idioma es este?', freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_language('What language is this?', freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'udm'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_language('Лэйкаса кадь лоба', freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_language('Які гэта мова?', freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         be       0.98      0.97      0.97        94\n",
      "         de       0.99      0.99      0.99        93\n",
      "         en       0.99      0.99      0.99        95\n",
      "         es       0.99      0.98      0.98        96\n",
      "         ru       0.93      0.93      0.93        90\n",
      "        tyv       0.96      0.91      0.93       100\n",
      "        udm       0.91      0.97      0.94        99\n",
      "\n",
      "avg / total       0.96      0.96      0.96       667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for lang in langs:\n",
    "    for text in wiki_texts[lang]:\n",
    "        true_labels.append(lang)\n",
    "        predicted_labels.append(predict_language(text, freq_dict))\n",
    "\n",
    "print(classification_report(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[91  0  0  0  1  0  2]\n",
      " [ 0 92  0  1  0  0  0]\n",
      " [ 0  0 94  0  0  1  0]\n",
      " [ 0  1  0 94  0  1  0]\n",
      " [ 1  0  1  0 84  2  2]\n",
      " [ 0  0  0  0  3 91  6]\n",
      " [ 1  0  0  0  2  0 96]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Частотные символьные n-граммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import islice, tee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция, которая преобразовывает строку в массив n-грамм заданной длины:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_ngrams(text):\n",
    "    N = 3 # задаем длину n-граммы\n",
    "    text = clean_data(text)\n",
    "    ngrams = zip(*(islice(seq, index, None) for index, seq in enumerate(tee(text, N))))\n",
    "    ngrams = [''.join(x) for x in ngrams]\n",
    "    \n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее -- аналогично первому методу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_ngrams = {}\n",
    "\n",
    "for lang in langs:\n",
    "    freq_ngrams[lang] = collections.defaultdict(lambda: 0)\n",
    "    for lang_text in wiki_texts[lang]:\n",
    "        for word in make_ngrams(lang_text.replace('\\n', '').lower()):\n",
    "            freq_ngrams[lang][word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for lang in langs:\n",
    "    for word in freq_ngrams[lang]:\n",
    "        freq_ngrams[lang][word] = freq_ngrams[lang][word] / sum(freq_ngrams[lang].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.169863439220805\tйзі\n",
      "0.0962968477358236\tфіс\n",
      "0.08852668307057804\t оф\n",
      "0.08187220761923117\tцяя\n",
      "0.07611484161010595\tзій\n",
      "0.07108859738186787\tsi \n",
      "0.06666538579300117\tcsi\n",
      "0.0627449276704814\tucs\n",
      "0.06193095967164124\tйму\n",
      "0.05924768854240502\t uc\n",
      "0.05610984052613308\tт u\n",
      "0.05334489065943288\tшоу\n",
      "0.053279610052401165\tkli\n",
      "0.050714590012080896\t kl\n",
      "0.05070059134253627\tоу \n",
      "0.04879794333453714\tump\n",
      "0.04657796803227206\tlum\n",
      "0.04080379508390425\tʊr \n",
      "0.039264559485277614\tpʊr\n",
      "0.037835184882437735\tmpʊ\n"
     ]
    }
   ],
   "source": [
    "for word in sorted(freq_ngrams['be'], key=lambda w: freq_ngrams['be'][w], reverse=True)[:20]:\n",
    "    print('{}\\t{}'.format(freq_ngrams['be'][word], word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_language_ngrams(text, freq_ngrams):\n",
    "    words_freq_ngrams = {}\n",
    "    \n",
    "    test_text_ngrams = make_ngrams(text)\n",
    "\n",
    "    for lang in langs:\n",
    "        i = 0\n",
    "        for word in test_text_ngrams:\n",
    "            if word in freq_ngrams[lang]:\n",
    "                i += freq_ngrams[lang][word]\n",
    "                \n",
    "        words_freq_ngrams[lang] = i   \n",
    "    \n",
    "    return max(words_freq_ngrams, key=words_freq_ngrams.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_language_ngrams('What language is this?', freq_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'es'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_language_ngrams('¿Qué idioma es este?', freq_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'de'"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_language_ngrams('Ich wohne in Moskau.', freq_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         be       0.96      0.99      0.97        94\n",
      "         de       0.98      1.00      0.99        93\n",
      "         en       0.98      1.00      0.99        95\n",
      "         es       0.96      0.99      0.97        96\n",
      "         ru       0.90      0.97      0.93        90\n",
      "        tyv       0.98      0.95      0.96       100\n",
      "        udm       0.95      0.82      0.88        99\n",
      "\n",
      "avg / total       0.96      0.96      0.96       667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for lang in langs:\n",
    "    for text in wiki_texts[lang]:\n",
    "        true_labels.append(lang)\n",
    "        predicted_labels.append(predict_language_ngrams(text, freq_ngrams))\n",
    "\n",
    "print(classification_report(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[93  0  0  0  1  0  0]\n",
      " [ 0 93  0  0  0  0  0]\n",
      " [ 0  0 95  0  0  0  0]\n",
      " [ 0  0  0 95  0  0  1]\n",
      " [ 1  0  0  1 87  0  1]\n",
      " [ 0  0  0  0  3 95  2]\n",
      " [ 3  2  2  3  6  2 81]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Вариант с семинара (работает лучше)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# посчитаем частотность нграммов\n",
    "lang2char_ngrams_freqs = collections.defaultdict(Counter)\n",
    "\n",
    "for lang in langs:\n",
    "    for text in wiki_texts[lang]:\n",
    "        char_ngrams = make_ngrams(text)\n",
    "        lang2char_ngrams_freqs[lang].update(char_ngrams)\n",
    "\n",
    "lang2char_ngrams = {}\n",
    "for lang in lang2char_ngrams_freqs:\n",
    "    topn = [word for word, freq in lang2char_ngrams_freqs[lang].most_common(500)] # с топ-300 качество хуже\n",
    "    lang2char_ngrams[lang] = set(topn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_language_ngrams(text, lang2char):\n",
    "    text_ngrams = make_ngrams(text)\n",
    "    \n",
    "    lang2sim = {}\n",
    "    \n",
    "    for lang in lang2char:\n",
    "        intersect = len(set(text_ngrams) & lang2char[lang])\n",
    "        lang2sim[lang] = intersect\n",
    "    \n",
    "    return max(lang2sim, key=lambda x: lang2sim[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for lang in langs:\n",
    "    for text in wiki_texts[lang]:\n",
    "        true_labels.append(lang)\n",
    "        predicted_labels.append(predict_language_ngrams(text, lang2char_ngrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         be       1.00      1.00      1.00        94\n",
      "         de       1.00      1.00      1.00        93\n",
      "         en       0.98      1.00      0.99        95\n",
      "         es       0.97      1.00      0.98        96\n",
      "         ru       0.91      0.99      0.95        90\n",
      "        tyv       0.98      0.93      0.95       100\n",
      "        udm       1.00      0.92      0.96        99\n",
      "\n",
      "avg / total       0.98      0.98      0.98       667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[94  0  0  0  0  0  0]\n",
      " [ 0 93  0  0  0  0  0]\n",
      " [ 0  0 95  0  0  0  0]\n",
      " [ 0  0  0 96  0  0  0]\n",
      " [ 0  0  1  0 89  0  0]\n",
      " [ 0  0  0  1  6 93  0]\n",
      " [ 0  0  1  2  3  2 91]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**В целом**, получаем, что оба алгоритма на данном наборе языков работают неплохо, однако метод с n-граммами оказывается лучше (особенно для некоторых из выбранных языков)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
